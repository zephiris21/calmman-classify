# affectnet_simple_training.py
# AffectNet ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ í™œìš©í•œ ë‹¨ìˆœí™”ëœ ì–¼êµ´ í‘œì • ì´ì§„ë¶„ë¥˜
# ì „ì²˜ë¦¬ëœ 260x260 ì •ë ¬ ì´ë¯¸ì§€ ì‚¬ìš©

import os
import numpy as np
import random
from PIL import Image
from tqdm import tqdm
import time
from pathlib import Path

# PyTorch ê´€ë ¨
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import GradScaler, autocast
from torchvision import transforms

# ë¶„ì„ ë° ì‹œê°í™”
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score
import matplotlib.pyplot as plt
import seaborn as sns
import json
from datetime import datetime

# ëœë¤ ì‹œë“œ ê³ ì •
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
random.seed(SEED)

# CuDNN ê²°ì •ì  ë™ì‘ ì„¤ì •
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print("=== ğŸš€ AffectNet ê¸°ë°˜ ì–¼êµ´ í‘œì • ì´ì§„ë¶„ë¥˜ (ë‹¨ìˆœí™” ë²„ì „) ===")
print("ì „ì²˜ë¦¬ëœ 260x260 ì •ë ¬ ì´ë¯¸ì§€ â†’ AffectNet íŠ¹ì§• â†’ ë¶„ë¥˜")

# =================================
# GPU ì„¤ì • ë° ìµœì í™”
# =================================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\n=== ğŸ”§ GPU ì„¤ì • ===")
print(f"Device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name()}")
    print(f"CUDA ë²„ì „: {torch.version.cuda}")
    print(f"ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    torch.cuda.empty_cache()
    print("âœ… CUDA ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

# =================================
# AffectNet ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë”©
# =================================
def load_affectnet_model(model_path, device):
    """AffectNet ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë”©"""
    print(f"\n=== ğŸ¤– AffectNet ëª¨ë¸ ë¡œë”© ===")
    print(f"ëª¨ë¸ ê²½ë¡œ: {model_path}")
    
    try:
        # ëª¨ë¸ ë¡œë”©
        affectnet_model = torch.load(model_path, map_location=device, weights_only=False)
        affectnet_model.eval()
        print("âœ… AffectNet ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
        
        # ëª¨ë¸ ì •ë³´ í™•ì¸
        if hasattr(affectnet_model, 'classifier'):
            classifier = affectnet_model.classifier
            feature_dim = classifier.in_features
            num_classes = classifier.out_features
            print(f"   - íŠ¹ì§• ì°¨ì›: {feature_dim}")
            print(f"   - ì›ë³¸ í´ë˜ìŠ¤: {num_classes}")
        else:
            raise ValueError("ë¶„ë¥˜ê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return affectnet_model, feature_dim
        
    except Exception as e:
        print(f"âŒ AffectNet ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise

# AffectNet ëª¨ë¸ ë¡œë”©
affectnet_model_path = "./models/affectnet_emotions/enet_b2_8.pt"
affectnet_model, feature_dim = load_affectnet_model(affectnet_model_path, device)

# =================================
# ë‹¨ìˆœí™”ëœ Dataset í´ë˜ìŠ¤
# =================================
class SimpleAffectNetDataset(Dataset):
    """ì „ì²˜ë¦¬ëœ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ìœ„í•œ ë‹¨ìˆœ Dataset"""
    
    def __init__(self, image_paths, labels, transform=None):
        """
        Args:
            image_paths (list): ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            labels (list): ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸
            transform (transforms): ë°ì´í„° ë³€í™˜
        """
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]
        
        try:
            # ì „ì²˜ë¦¬ëœ 260x260 ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(img_path).convert('RGB')
            
            # Transform ì ìš©
            if self.transform:
                image = self.transform(image)
            
            return image, torch.tensor(label, dtype=torch.long)
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ {img_path}: {e}")
            # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ì´ë¯¸ì§€ ë°˜í™˜
            dummy_image = torch.zeros(3, 260, 260)
            return dummy_image, torch.tensor(label, dtype=torch.long)

# =================================
# AffectNet ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ ì •ì˜ (ìˆ˜ì •ëœ ë²„ì „)
# =================================
class AffectNetBinaryClassifier(nn.Module):
    """AffectNet ì‚¬ì „í•™ìŠµ ëª¨ë¸ ê¸°ë°˜ ì´ì§„ë¶„ë¥˜ê¸° (ìˆ˜ì •ëœ ë²„ì „)"""
    
    def __init__(self, affectnet_model, feature_dim=1408, num_classes=2, dropout_rate=0.3):
        super(AffectNetBinaryClassifier, self).__init__()
        
        # AffectNet ë°±ë³¸ì—ì„œ classifier ì œê±°
        self.backbone = nn.ModuleList(list(affectnet_model.children())[:-1])
        
        # ìƒˆë¡œìš´ ì´ì§„ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(feature_dim, num_classes)
        )
        
        # íŠ¹ì§• ì°¨ì› ì €ì¥
        self.feature_dim = feature_dim
        
        print(f"âœ… AffectNet ì´ì§„ë¶„ë¥˜ê¸° ìƒì„± ì™„ë£Œ")
        print(f"   - ë°±ë³¸: AffectNet EfficientNet-B2")
        print(f"   - íŠ¹ì§• ì°¨ì›: {feature_dim}")
        print(f"   - ë¶„ë¥˜ í—¤ë“œ: Dropout({dropout_rate}) â†’ Linear({feature_dim}, {num_classes})")
        
    def forward(self, x):
        # ë°±ë³¸ì„ í†µê³¼í•˜ì—¬ íŠ¹ì§• ì¶”ì¶œ
        for module in self.backbone:
            x = module(x)
        
        # ì´ë¯¸ Global Poolingì´ ì ìš©ëœ ìƒíƒœë¼ê³  ê°€ì •
        # x shape: [batch_size, feature_dim]
        
        # ë§Œì•½ 4D tensorë¼ë©´ flatten
        if len(x.shape) > 2:
            x = torch.flatten(x, 1)
        
        # ë¶„ë¥˜
        outputs = self.classifier(x)
        return outputs
    
    def extract_features(self, x):
        """íŠ¹ì§• ë²¡í„°ë§Œ ì¶”ì¶œ (ë¶„ë¥˜ í—¤ë“œ ì œì™¸)"""
        with torch.no_grad():
            for module in self.backbone:
                x = module(x)
            
            if len(x.shape) > 2:
                x = torch.flatten(x, 1)
            
            return x
    
    def freeze_backbone(self):
        """ë°±ë³¸ ê°€ì¤‘ì¹˜ ë™ê²°"""
        for module in self.backbone:
            for param in module.parameters():
                param.requires_grad = False
        print("ğŸ”’ AffectNet ë°±ë³¸ ê°€ì¤‘ì¹˜ ë™ê²°ë¨")
    
    def unfreeze_backbone(self, layers_to_unfreeze=3):
        """ë°±ë³¸ ì¼ë¶€ ë ˆì´ì–´ í•´ì œ"""
        # ëª¨ë“  ë°±ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì¼ë‹¨ ë™ê²°
        for module in self.backbone:
            for param in module.parameters():
                param.requires_grad = False
        
        # ë§ˆì§€ë§‰ ëª‡ ê°œ ëª¨ë“ˆë§Œ í•´ì œ
        total_modules = len(self.backbone)
        unfreeze_from = max(0, total_modules - layers_to_unfreeze)
        
        for i in range(unfreeze_from, total_modules):
            for param in self.backbone[i].parameters():
                param.requires_grad = True
        
        print(f"ğŸ”“ ë°±ë³¸ ë§ˆì§€ë§‰ {layers_to_unfreeze}ê°œ ëª¨ë“ˆ í•´ì œë¨ ({unfreeze_from}ë²ˆë¶€í„°)")
        
        # ë¶„ë¥˜ í—¤ë“œëŠ” í•­ìƒ í•™ìŠµ ê°€ëŠ¥
        for param in self.classifier.parameters():
            param.requires_grad = True

# =================================
# ëª¨ë¸ ìƒì„± ë° GPU ì´ë™
# =================================
print(f"\n=== ğŸ—ï¸ ëª¨ë¸ ìƒì„± ===")

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model = AffectNetBinaryClassifier(
    affectnet_model=affectnet_model,
    feature_dim=feature_dim,
    num_classes=2,
    dropout_rate=0.3
)

# GPUë¡œ ì´ë™
model = model.to(device)

# ëª¨ë¸ ì •ë³´ ì¶œë ¥
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
print(f"   - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
print(f"   - í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
print(f"   - íŠ¹ì§• ì°¨ì›: {model.feature_dim}")

# =================================
# Transform ì •ì˜
# =================================
print(f"\n=== ğŸ”„ Transform ì„¤ì • ===")

# í›ˆë ¨ìš© Transform (ë°ì´í„° ì¦ê°•)
train_transform = transforms.Compose([
    transforms.Resize((260, 260)),  # ì´ë¯¸ 260ì´ì§€ë§Œ ì•ˆì „ì„ ìœ„í•´
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
    transforms.RandomRotation(degrees=5),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet ì •ê·œí™”
])

# ê²€ì¦ìš© Transform (ì¦ê°• ì—†ìŒ)
val_transform = transforms.Compose([
    transforms.Resize((260, 260)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

print("âœ… Transform ì„¤ì • ì™„ë£Œ")
print("   - í›ˆë ¨ìš©: Resize(260) + RandomFlip + ColorJitter + Rotation + ì •ê·œí™”")
print("   - ê²€ì¦ìš©: Resize(260) + ì •ê·œí™”ë§Œ")

# =================================
# ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# =================================
def collect_preprocessed_images(folder_path, label, extensions=('.jpg', '.jpeg', '.png')):
    """ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘"""
    image_paths = []
    labels = []
    
    if not os.path.exists(folder_path):
        print(f"âŒ {folder_path} í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return image_paths, labels
    
    all_files = os.listdir(folder_path)
    image_files = [f for f in all_files if f.lower().endswith(extensions)]
    
    print(f"ğŸ“ {os.path.basename(folder_path)} í´ë”:")
    print(f"   ì „ì²´ íŒŒì¼: {len(all_files)}ê°œ")
    print(f"   ì´ë¯¸ì§€ íŒŒì¼: {len(image_files)}ê°œ")
    
    for fname in image_files:
        img_path = os.path.join(folder_path, fname)
        image_paths.append(img_path)
        labels.append(label)
    
    return image_paths, labels

# ì „ì²˜ë¦¬ëœ ë°ì´í„° ê²½ë¡œ ì„¤ì •
processed_base_path = r'D:\my_projects\calmman-facial-classification\data\affectnet_processed'
teasing_processed_path = os.path.join(processed_base_path, 'teasing')
non_teasing_processed_path = os.path.join(processed_base_path, 'non_teasing')

print("\n=== ğŸ“ ì „ì²˜ë¦¬ëœ ë°ì´í„° ìˆ˜ì§‘ ===")
# ë¹„ì•½ì˜¬ë¦¬ê¸° ì „ì²˜ë¦¬ ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘ (ë¼ë²¨: 0)
X_non_teasing_paths, y_non_teasing = collect_preprocessed_images(non_teasing_processed_path, 0)

# ì•½ì˜¬ë¦¬ê¸° ì „ì²˜ë¦¬ ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘ (ë¼ë²¨: 1)
X_teasing_paths, y_teasing = collect_preprocessed_images(teasing_processed_path, 1)

# ë°ì´í„° í•©ì¹˜ê¸°
all_image_paths = X_non_teasing_paths + X_teasing_paths
all_labels = y_non_teasing + y_teasing

print(f"ì „ì²˜ë¦¬ëœ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ:")
print(f"  ë¹„ì•½ì˜¬ë¦¬ê¸°: {len(X_non_teasing_paths)}ê°œ")
print(f"  ì•½ì˜¬ë¦¬ê¸°: {len(X_teasing_paths)}ê°œ")
print(f"  ì´ ì´ë¯¸ì§€: {len(all_image_paths)}ê°œ")

if len(all_image_paths) == 0:
    print("âŒ ìˆ˜ì§‘ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
    print("ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”!")
    exit()

# =================================
# ë°ì´í„° ë¶„í• 
# =================================
print(f"\n=== âœ‚ï¸ ë°ì´í„° ë¶„í•  ===")

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
y_array = np.array(all_labels)
print(f"í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_array)}")

# í›ˆë ¨/ê²€ì¦ ë¶„í• 
X_train_paths, X_val_paths, y_train, y_val = train_test_split(
    all_image_paths, all_labels, 
    test_size=0.2, 
    random_state=SEED, 
    stratify=all_labels
)

print(f"í›ˆë ¨ ë°ì´í„°: {len(X_train_paths)}ê°œ")
print(f"ê²€ì¦ ë°ì´í„°: {len(X_val_paths)}ê°œ")
print(f"í›ˆë ¨ í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_train)}")
print(f"ê²€ì¦ í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_val)}")

# =================================
# Dataset ë° DataLoader ìƒì„±
# =================================
print(f"\n=== ğŸ“¦ Dataset ë° DataLoader ìƒì„± ===")

# Dataset ìƒì„±
train_dataset = SimpleAffectNetDataset(X_train_paths, y_train, transform=train_transform)
val_dataset = SimpleAffectNetDataset(X_val_paths, y_val, transform=val_transform)

print(f"âœ… Dataset ìƒì„± ì™„ë£Œ")
print(f"   - í›ˆë ¨ Dataset: {len(train_dataset)}ê°œ")
print(f"   - ê²€ì¦ Dataset: {len(val_dataset)}ê°œ")

# DataLoader ìƒì„±
batch_size = 32  # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¡œ ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê°€ëŠ¥
num_workers = 0  # Windows í˜¸í™˜

train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
    pin_memory=True,
    drop_last=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=num_workers,
    pin_memory=True
)

print(f"âœ… DataLoader ìƒì„± ì™„ë£Œ")
print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
print(f"   - í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
print(f"   - ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}")

# =================================
# 1ë‹¨ê³„ í•™ìŠµ ì„¤ì • (ë°±ë³¸ ë™ê²°)
# =================================
print(f"\n=== ğŸ¯ 1ë‹¨ê³„ í•™ìŠµ ì„¤ì • (ë°±ë³¸ ë™ê²°) ===")

# ë°±ë³¸ ë™ê²°
model.freeze_backbone()

# 1ë‹¨ê³„ ì†ì‹¤í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
criterion = nn.CrossEntropyLoss()
optimizer_stage1 = optim.Adam(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=0.001,
    weight_decay=1e-4
)

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
scheduler_stage1 = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer_stage1,
    mode='min',
    factor=0.5,
    patience=3,
    min_lr=1e-7
)

# Mixed Precision ìŠ¤ì¼€ì¼ëŸ¬
scaler = GradScaler()

print("âœ… 1ë‹¨ê³„ í•™ìŠµ ì„¤ì • ì™„ë£Œ")
print(f"   - ì†ì‹¤í•¨ìˆ˜: CrossEntropyLoss")
print(f"   - ì˜µí‹°ë§ˆì´ì €: Adam (lr=0.001)")
print(f"   - ìŠ¤ì¼€ì¤„ëŸ¬: ReduceLROnPlateau")
print(f"   - Mixed Precision: í™œì„±í™”")

# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸
trainable_params_stage1 = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"   - 1ë‹¨ê³„ í•™ìŠµ íŒŒë¼ë¯¸í„°: {trainable_params_stage1:,}")

# =================================
# í•™ìŠµ ë° ê²€ì¦ í•¨ìˆ˜
# =================================
def train_epoch(model, train_loader, criterion, optimizer, scaler, device):
    """í•œ ì—í¬í¬ í›ˆë ¨"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(train_loader, desc="Training")
    for batch_idx, (images, labels) in enumerate(pbar):
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        
        # Mixed Precision Training
        with autocast():
            outputs = model(images)
            loss = criterion(outputs, labels)
        
        # Backward pass
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        # í†µê³„ ê³„ì‚°
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        pbar.set_postfix({
            'Loss': f'{running_loss/(batch_idx+1):.4f}',
            'Acc': f'{100.*correct/total:.2f}%'
        })
    
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

def validate_epoch(model, val_loader, criterion, device):
    """í•œ ì—í¬í¬ ê²€ì¦"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        pbar = tqdm(val_loader, desc="Validation")
        for batch_idx, (images, labels) in enumerate(pbar):
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            # ì˜ˆì¸¡ê°’ ì €ì¥
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            pbar.set_postfix({
                'Loss': f'{running_loss/(batch_idx+1):.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
    
    epoch_loss = running_loss / len(val_loader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc, all_predictions, all_labels

# =================================
# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
# =================================
# ê²°ê³¼ ì €ì¥ ê²½ë¡œ
results_base = "results/affectnet_simple"
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# ë””ë ‰í† ë¦¬ ìƒì„±
dirs_to_create = [
    f"{results_base}/models",
    f"{results_base}/plots", 
    f"{results_base}/metrics",
    f"{results_base}/logs"
]

for dir_path in dirs_to_create:
    os.makedirs(dir_path, exist_ok=True)

print(f"\n=== ğŸ“ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± ===")
print(f"ê¸°ë³¸ ê²½ë¡œ: {results_base}")

# =================================
# 1ë‹¨ê³„ í•™ìŠµ ì‹¤í–‰
# =================================
print(f"\n=== ğŸš€ 1ë‹¨ê³„ í•™ìŠµ: ë°±ë³¸ ë™ê²° + ë¶„ë¥˜ í—¤ë“œ í•™ìŠµ ===")

# 1ë‹¨ê³„ ì„¤ì •
stage1_epochs = 15
best_val_loss = float('inf')
patience_counter = 0
patience = 5

print(f"1ë‹¨ê³„ í•™ìŠµ ì‹œì‘:")
print(f"  - ì—í¬í¬: {stage1_epochs}")
print(f"  - ë°±ë³¸: ì™„ì „ ë™ê²°")
print(f"  - í•™ìŠµë¥ : {optimizer_stage1.param_groups[0]['lr']}")
print(f"  - ì¡°ê¸° ì¢…ë£Œ: patience={patience}")

# 1ë‹¨ê³„ í•™ìŠµ ê¸°ë¡
stage1_history = {
    'train_loss': [], 'train_acc': [],
    'val_loss': [], 'val_acc': []
}

for epoch in range(stage1_epochs):
    print(f"\nğŸ“Š Epoch {epoch+1}/{stage1_epochs}")
    
    # í›ˆë ¨
    train_loss, train_acc = train_epoch(
        model, train_loader, criterion, optimizer_stage1, scaler, device
    )
    
    # ê²€ì¦
    val_loss, val_acc, val_predictions, val_labels = validate_epoch(
        model, val_loader, criterion, device
    )
    
    # ê¸°ë¡ ì €ì¥
    stage1_history['train_loss'].append(train_loss)
    stage1_history['train_acc'].append(train_acc)
    stage1_history['val_loss'].append(val_loss)
    stage1_history['val_acc'].append(val_acc)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
    scheduler_stage1.step(val_loss)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"ğŸ’¯ Results - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
    print(f"ğŸ’¯ Results - Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
    print(f"ğŸ“š LR: {optimizer_stage1.param_groups[0]['lr']:.6f}")
    
    # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        # ìµœê³  ëª¨ë¸ ì €ì¥
        stage1_model_path = f"{results_base}/models/best_model_stage1_{timestamp}.pth"
        torch.save(model.state_dict(), stage1_model_path)
        print(f"ğŸ’¾ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨")
    else:
        patience_counter += 1
        print(f"â° Patience: {patience_counter}/{patience}")
        
        if patience_counter >= patience:
            print("ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ!")
            break

# ìµœê³  ëª¨ë¸ ë¡œë“œ
stage1_model_path = f"{results_base}/models/best_model_stage1_{timestamp}.pth"
checkpoint = torch.load(stage1_model_path, weights_only=False)

# OrderedDictì¸ì§€ ì „ì²´ ëª¨ë¸ì¸ì§€ í™•ì¸
if isinstance(checkpoint, dict):
    # state_dictì¸ ê²½ìš°
    model.load_state_dict(checkpoint)
else:
    # ì „ì²´ ëª¨ë¸ì¸ ê²½ìš°
    model = checkpoint

print(f"\nâœ… 1ë‹¨ê³„ ì™„ë£Œ! ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")

# =================================
# 2ë‹¨ê³„ í•™ìŠµ: ë°±ë³¸ ì¼ë¶€ í•´ì œ + ë¯¸ì„¸ì¡°ì •
# =================================
print(f"\n=== ğŸ”¬ 2ë‹¨ê³„ í•™ìŠµ: ë°±ë³¸ ì¼ë¶€ í•´ì œ + ë¯¸ì„¸ì¡°ì • ===")

# ë°±ë³¸ ì¼ë¶€ í•´ì œ
model.unfreeze_backbone(layers_to_unfreeze=3)

# 2ë‹¨ê³„ ì˜µí‹°ë§ˆì´ì € (ë” ì‘ì€ í•™ìŠµë¥ )
optimizer_stage2 = optim.Adam(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=0.0001,  # 10ë°° ì‘ì€ í•™ìŠµë¥ 
    weight_decay=1e-4
)

# 2ë‹¨ê³„ ìŠ¤ì¼€ì¤„ëŸ¬
scheduler_stage2 = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer_stage2,
    mode='min',
    factor=0.3,
    patience=3,
    min_lr=1e-8
)

# 2ë‹¨ê³„ ì„¤ì •
stage2_epochs = 20
best_val_loss_stage2 = float('inf')
patience_counter_stage2 = 0
patience_stage2 = 7

# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸
trainable_params_stage2 = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"2ë‹¨ê³„ í•™ìŠµ ì‹œì‘:")
print(f"  - ì—í¬í¬: {stage2_epochs}")
print(f"  - í•™ìŠµ íŒŒë¼ë¯¸í„°: {trainable_params_stage2:,}")
print(f"  - í•™ìŠµë¥ : {optimizer_stage2.param_groups[0]['lr']}")
print(f"  - ì¡°ê¸° ì¢…ë£Œ: patience={patience_stage2}")

# 2ë‹¨ê³„ í•™ìŠµ ê¸°ë¡
stage2_history = {
    'train_loss': [], 'train_acc': [],
    'val_loss': [], 'val_acc': []
}

for epoch in range(stage2_epochs):
    print(f"\nğŸ“Š Stage2 Epoch {epoch+1}/{stage2_epochs}")
    
    # í›ˆë ¨
    train_loss, train_acc = train_epoch(
        model, train_loader, criterion, optimizer_stage2, scaler, device
    )
    
    # ê²€ì¦
    val_loss, val_acc, val_predictions, val_labels = validate_epoch(
        model, val_loader, criterion, device
    )
    
    # ê¸°ë¡ ì €ì¥
    stage2_history['train_loss'].append(train_loss)
    stage2_history['train_acc'].append(train_acc)
    stage2_history['val_loss'].append(val_loss)
    stage2_history['val_acc'].append(val_acc)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
    scheduler_stage2.step(val_loss)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"ğŸ’¯ Results - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
    print(f"ğŸ’¯ Results - Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
    print(f"ğŸ“š LR: {optimizer_stage2.param_groups[0]['lr']:.6f}")
    
    # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
    if val_loss < best_val_loss_stage2:
        best_val_loss_stage2 = val_loss
        patience_counter_stage2 = 0
        # ìµœê³  ëª¨ë¸ ì €ì¥
        stage2_model_path = f"{results_base}/models/best_model_stage2_{timestamp}.pth"
        torch.save(model.state_dict(), stage2_model_path)
        print(f"ğŸ’¾ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥ë¨")
    else:
        patience_counter_stage2 += 1
        print(f"â° Patience: {patience_counter_stage2}/{patience_stage2}")
        
        if patience_counter_stage2 >= patience_stage2:
            print("ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ!")
            break

# ìµœê³  ëª¨ë¸ ë¡œë“œ
stage2_model_path = f"{results_base}/models/best_model_stage2_{timestamp}.pth"
checkpoint = torch.load(stage2_model_path, weights_only=False)

# OrderedDictì¸ì§€ ì „ì²´ ëª¨ë¸ì¸ì§€ í™•ì¸
if isinstance(checkpoint, dict):
    # state_dictì¸ ê²½ìš°
    model.load_state_dict(checkpoint)
else:
    # ì „ì²´ ëª¨ë¸ì¸ ê²½ìš°
    model = checkpoint

print(f"\nâœ… 2ë‹¨ê³„ ì™„ë£Œ! ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss_stage2:.4f}")

# =================================
# ìµœì¢… ì„±ëŠ¥ í‰ê°€
# =================================
print(f"\n=== ğŸ“Š ìµœì¢… ì„±ëŠ¥ í‰ê°€ ===")

# ìµœì¢… ê²€ì¦ ìˆ˜í–‰
model.eval()
final_val_loss, final_val_acc, final_predictions, final_labels = validate_epoch(
    model, val_loader, criterion, device
)

print(f"ìµœì¢… ê²€ì¦ ê²°ê³¼:")
print(f"  - ê²€ì¦ ì •í™•ë„: {final_val_acc:.2f}%")
print(f"  - ê²€ì¦ ì†ì‹¤: {final_val_loss:.4f}")

# F1 Score ê³„ì‚°
f1 = f1_score(final_labels, final_predictions)
precision = precision_score(final_labels, final_predictions)
recall = recall_score(final_labels, final_predictions)

print(f"  - F1 Score: {f1:.4f}")
print(f"  - Precision: {precision:.4f}")
print(f"  - Recall: {recall:.4f}")

# ë¶„ë¥˜ ë³´ê³ ì„œ
classes = ['non_teasing', 'teasing']
print(f"\në¶„ë¥˜ ë³´ê³ ì„œ:")
print(classification_report(final_labels, final_predictions, target_names=classes))

# í˜¼ë™ í–‰ë ¬
cm = confusion_matrix(final_labels, final_predictions)
print(f"\ní˜¼ë™ í–‰ë ¬:")
print(cm)

# =================================
# ê²°ê³¼ ì‹œê°í™”
# =================================
print(f"\n=== ğŸ“ˆ ê²°ê³¼ ì‹œê°í™” ===")

# ì „ì²´ í•™ìŠµ ê¸°ë¡ í•©ì¹˜ê¸°
total_epochs_stage1 = len(stage1_history['train_loss'])
total_epochs_stage2 = len(stage2_history['train_loss'])

# ì—°ì†ëœ ì—í¬í¬ë¡œ ë³€í™˜
all_train_loss = stage1_history['train_loss'] + stage2_history['train_loss']
all_train_acc = stage1_history['train_acc'] + stage2_history['train_acc']
all_val_loss = stage1_history['val_loss'] + stage2_history['val_loss']
all_val_acc = stage1_history['val_acc'] + stage2_history['val_acc']

# ì—í¬í¬ ë²”ìœ„
epochs_stage1 = list(range(1, total_epochs_stage1 + 1))
epochs_stage2 = list(range(total_epochs_stage1 + 1, total_epochs_stage1 + total_epochs_stage2 + 1))
all_epochs = epochs_stage1 + epochs_stage2

# ì‹œê°í™”
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('AffectNet-based Binary Classification Results (Simple Version)', fontsize=16)

# 1. ì •í™•ë„ ê·¸ë˜í”„
axes[0,0].plot(epochs_stage1, stage1_history['train_acc'], 'b-', label='Stage 1 Training', linewidth=2)
axes[0,0].plot(epochs_stage1, stage1_history['val_acc'], 'b--', label='Stage 1 Validation', linewidth=2)
axes[0,0].plot(epochs_stage2, stage2_history['train_acc'], 'g-', label='Stage 2 Training', linewidth=2)
axes[0,0].plot(epochs_stage2, stage2_history['val_acc'], 'g--', label='Stage 2 Validation', linewidth=2)
axes[0,0].axhline(y=50, color='gray', linestyle=':', alpha=0.7, label='Random Baseline')
axes[0,0].axvline(x=total_epochs_stage1, color='red', linestyle=':', alpha=0.7, label='Stage Transition')
axes[0,0].set_title('Model Accuracy (AffectNet 2-Stage Training)')
axes[0,0].set_xlabel('Epoch')
axes[0,0].set_ylabel('Accuracy (%)')
axes[0,0].legend()
axes[0,0].grid(True, alpha=0.3)

# 2. ì†ì‹¤ ê·¸ë˜í”„
axes[0,1].plot(epochs_stage1, stage1_history['train_loss'], 'b-', label='Stage 1 Training', linewidth=2)
axes[0,1].plot(epochs_stage1, stage1_history['val_loss'], 'b--', label='Stage 1 Validation', linewidth=2)
axes[0,1].plot(epochs_stage2, stage2_history['train_loss'], 'g-', label='Stage 2 Training', linewidth=2)
axes[0,1].plot(epochs_stage2, stage2_history['val_loss'], 'g--', label='Stage 2 Validation', linewidth=2)
axes[0,1].axvline(x=total_epochs_stage1, color='red', linestyle=':', alpha=0.7, label='Stage Transition')
axes[0,1].set_title('Model Loss (AffectNet 2-Stage Training)')
axes[0,1].set_xlabel('Epoch')
axes[0,1].set_ylabel('Loss')
axes[0,1].legend()
axes[0,1].grid(True, alpha=0.3)

# 3. í˜¼ë™ í–‰ë ¬ íˆíŠ¸ë§µ
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
           xticklabels=classes, yticklabels=classes, ax=axes[1,0])
axes[1,0].set_title('Confusion Matrix')
axes[1,0].set_xlabel('Predicted')
axes[1,0].set_ylabel('Actual')

# 4. ì˜ˆì¸¡ í™•ë¥  ë¶„í¬
model.eval()
with torch.no_grad():
    # ìƒ˜í”Œ ë°°ì¹˜ë¡œ í™•ë¥  ë¶„í¬ í™•ì¸
    sample_images, sample_labels = next(iter(val_loader))
    sample_images, sample_labels = sample_images.to(device), sample_labels.to(device)
    sample_outputs = model(sample_images)
    sample_probs = torch.softmax(sample_outputs, dim=1)[:, 1].cpu().numpy()  # teasing í´ë˜ìŠ¤ í™•ë¥ 
    sample_labels_cpu = sample_labels.cpu().numpy()

axes[1,1].hist(sample_probs[sample_labels_cpu==0], alpha=0.5, label='non_teasing', bins=20, color='blue')
axes[1,1].hist(sample_probs[sample_labels_cpu==1], alpha=0.5, label='teasing', bins=20, color='orange')
axes[1,1].axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold')
axes[1,1].set_title('Prediction Probability Distribution')
axes[1,1].set_xlabel('Predicted Probability (Teasing Class)')
axes[1,1].set_ylabel('Count')
axes[1,1].legend()
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()

# ê·¸ë˜í”„ ì €ì¥
plot_path = f"{results_base}/plots/affectnet_simple_results_{timestamp}.png"
plt.savefig(plot_path, dpi=300, bbox_inches='tight')
print(f"ğŸ“ˆ í•™ìŠµ ê²°ê³¼ ê·¸ë˜í”„ ì €ì¥: {plot_path}")

plt.show()

# =================================
# ì„±ëŠ¥ ê°œì„  ë¶„ì„
# =================================
print(f"\n=== ğŸ¯ ì„±ëŠ¥ ê°œì„  ë¶„ì„ ===")

# ê¸°ì¤€ì„ ê³¼ ë¹„êµ (ê¸°ì¡´ ImageNet ëª¨ë¸ ê¸°ì¤€)
baseline_accuracy = 77.5  # ê¸°ì¡´ ImageNet EfficientNet ì„±ëŠ¥
improvement = final_val_acc - baseline_accuracy
improvement_percent = (improvement / baseline_accuracy) * 100

print(f"âœ… AffectNet vs ImageNet ë¹„êµ:")
print(f"   - ê¸°ì¡´ ImageNet EfficientNet: {baseline_accuracy}%")
print(f"   - AffectNet EfficientNet: {final_val_acc:.1f}%")
print(f"   - ê°œì„ í­: {improvement:+.1f}% ({improvement_percent:+.1f}%)")
print(f"   - F1 Score: {f1:.4f}")
print(f"   - ì´ í•™ìŠµ ì—í¬í¬: {total_epochs_stage1 + total_epochs_stage2}íšŒ")

# ì„±ëŠ¥ í‰ê°€
if final_val_acc > 90:
    print(f"ğŸ‰ ë›°ì–´ë‚œ ì„±ëŠ¥! AffectNet ì‚¬ì „í•™ìŠµì´ ë§¤ìš° íš¨ê³¼ì ")
elif final_val_acc > 85:
    print(f"ğŸ‰ ëª©í‘œ ì´ˆê³¼ ë‹¬ì„±! AffectNet ì „ì´í•™ìŠµì´ ë§¤ìš° íš¨ê³¼ì ") 
elif final_val_acc > 82:
    print(f"ğŸ‘ ëª©í‘œ ë‹¬ì„±! AffectNet ì „ì´í•™ìŠµ íš¨ê³¼ í™•ì¸")
elif final_val_acc > baseline_accuracy:
    print(f"âœ¨ ì„±ëŠ¥ í–¥ìƒ! AffectNet ì‚¬ì „í•™ìŠµ ë„ì›€ë¨")
else:
    print(f"ğŸ¤” ì„±ëŠ¥ ì •ì²´. ì¶”ê°€ ì¡°ì • í•„ìš”")

# =================================
# ê²°ê³¼ ì €ì¥
# =================================
print(f"\n=== ğŸ’¾ ê²°ê³¼ ì €ì¥ ===")

# ìµœì¢… ëª¨ë¸ ì €ì¥
final_model_path = f"{results_base}/models/final_affectnet_simple_model_{timestamp}.pth"
torch.save(model.state_dict(), final_model_path)
print(f"ğŸ¤– ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")

# í•™ìŠµ ê¸°ë¡ ì €ì¥
training_log = {
    'timestamp': timestamp,
    'model_config': {
        'backbone': 'AffectNet_EfficientNet_B2',
        'feature_dim': feature_dim,
        'num_classes': 2,
        'dropout_rate': 0.3,
        'input_size': 260,
        'preprocessed': True
    },
    'training_config': {
        'stage1_epochs': total_epochs_stage1,
        'stage2_epochs': total_epochs_stage2,
        'batch_size': batch_size,
        'num_workers': num_workers,
        'mixed_precision': True,
        'face_alignment': True,
        'preprocessed_data': True
    },
    'final_metrics': {
        'val_accuracy': float(final_val_acc),
        'val_loss': float(final_val_loss),
        'f1_score': float(f1),
        'precision': float(precision),
        'recall': float(recall),
        'improvement_over_baseline': float(improvement)
    },
    'data_info': {
        'train_samples': len(X_train_paths),
        'val_samples': len(X_val_paths),
        'total_samples': len(all_image_paths)
    },
    'stage1_history': {
        'train_loss': [float(x) for x in stage1_history['train_loss']],
        'train_acc': [float(x) for x in stage1_history['train_acc']],
        'val_loss': [float(x) for x in stage1_history['val_loss']],
        'val_acc': [float(x) for x in stage1_history['val_acc']]
    },
    'stage2_history': {
        'train_loss': [float(x) for x in stage2_history['train_loss']],
        'train_acc': [float(x) for x in stage2_history['train_acc']],
        'val_loss': [float(x) for x in stage2_history['val_loss']],
        'val_acc': [float(x) for x in stage2_history['val_acc']]
    }
}

log_path = f"{results_base}/logs/affectnet_simple_training_log_{timestamp}.json"
with open(log_path, 'w', encoding='utf-8') as f:
    json.dump(training_log, f, indent=2, ensure_ascii=False)
print(f"ğŸ“Š í•™ìŠµ ë¡œê·¸ ì €ì¥: {log_path}")

# ì„±ëŠ¥ ì§€í‘œ ì €ì¥
metrics_text = f"""AffectNet ê¸°ë°˜ ì–¼êµ´ í‘œì • ì´ì§„ë¶„ë¥˜ ê²°ê³¼ (ë‹¨ìˆœí™” ë²„ì „)
================================================
ì‹¤í–‰ ì‹œê°„: {timestamp}

ìµœì¢… ì„±ëŠ¥:
- ê²€ì¦ ì •í™•ë„: {final_val_acc:.2f}%
- ê²€ì¦ ì†ì‹¤: {final_val_loss:.4f}
- F1 Score: {f1:.4f}
- Precision: {precision:.4f}
- Recall: {recall:.4f}

í•™ìŠµ ì„¤ì •:
- 1ë‹¨ê³„ ì—í¬í¬: {total_epochs_stage1}
- 2ë‹¨ê³„ ì—í¬í¬: {total_epochs_stage2}
- ì´ ì—í¬í¬: {total_epochs_stage1 + total_epochs_stage2}
- ë°°ì¹˜ í¬ê¸°: {batch_size}
- ì…ë ¥ í¬ê¸°: 260x260
- ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©: Yes
- ì–¼êµ´ ì •ë ¬: ì‚¬ì „ ì ìš©ë¨
- Mixed Precision: í™œì„±í™”

ë°ì´í„° ì •ë³´:
- í›ˆë ¨ ìƒ˜í”Œ: {len(X_train_paths)}ê°œ
- ê²€ì¦ ìƒ˜í”Œ: {len(X_val_paths)}ê°œ
- ì´ ìƒ˜í”Œ: {len(all_image_paths)}ê°œ

ê¸°ì¡´ ëŒ€ë¹„ ê°œì„ :
- ê¸°ì¡´ ImageNet EfficientNet: {baseline_accuracy}%
- AffectNet EfficientNet: {final_val_acc:.1f}%
- ê°œì„ í­: {improvement:+.1f}% ({improvement_percent:+.1f}%)

í˜¼ë™ í–‰ë ¬:
{cm}

ë¶„ë¥˜ ë³´ê³ ì„œ:
{classification_report(final_labels, final_predictions, target_names=classes)}
"""

metrics_path = f"{results_base}/metrics/affectnet_simple_performance_summary_{timestamp}.txt"
with open(metrics_path, 'w', encoding='utf-8') as f:
    f.write(metrics_text)
print(f"ğŸ“ˆ ì„±ëŠ¥ ìš”ì•½ ì €ì¥: {metrics_path}")

# =================================
# íŠ¹ì§• ë²¡í„° ë¶„ì„ (ì˜µì…˜)
# =================================
print(f"\n=== ğŸ”¬ íŠ¹ì§• ë²¡í„° ë¶„ì„ ===")

# ê²€ì¦ ë°ì´í„°ë¡œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
model.eval()
all_features = []
all_feature_labels = []

print("íŠ¹ì§• ë²¡í„° ì¶”ì¶œ ì¤‘...")
with torch.no_grad():
    for images, labels in tqdm(val_loader, desc="Extracting features"):
        images = images.to(device)
        features = model.extract_features(images)
        
        all_features.append(features.cpu().numpy())
        all_feature_labels.extend(labels.numpy())

# íŠ¹ì§• ë²¡í„° í•©ì¹˜ê¸°
all_features = np.vstack(all_features)
all_feature_labels = np.array(all_feature_labels)

print(f"ì¶”ì¶œëœ íŠ¹ì§• ë²¡í„° í˜•íƒœ: {all_features.shape}")
print(f"íŠ¹ì§• ë²¡í„° ë²”ìœ„: [{all_features.min():.3f}, {all_features.max():.3f}]")
print(f"íŠ¹ì§• ë²¡í„° í‰ê· : {all_features.mean():.3f}")
print(f"íŠ¹ì§• ë²¡í„° í‘œì¤€í¸ì°¨: {all_features.std():.3f}")

# í´ë˜ìŠ¤ë³„ íŠ¹ì§• í†µê³„
for class_idx, class_name in enumerate(classes):
    class_features = all_features[all_feature_labels == class_idx]
    print(f"{class_name} í´ë˜ìŠ¤:")
    print(f"  - ìƒ˜í”Œ ìˆ˜: {len(class_features)}")
    print(f"  - í‰ê· : {class_features.mean():.3f}")
    print(f"  - í‘œì¤€í¸ì°¨: {class_features.std():.3f}")

# íŠ¹ì§• ë²¡í„° ì €ì¥ (ë‚˜ì¤‘ì— ë¶„ì„ìš©)
features_save_path = f"{results_base}/features/extracted_features_{timestamp}.npz"
os.makedirs(os.path.dirname(features_save_path), exist_ok=True)
np.savez(features_save_path, 
         features=all_features, 
         labels=all_feature_labels, 
         class_names=classes)
print(f"ğŸ”¬ íŠ¹ì§• ë²¡í„° ì €ì¥: {features_save_path}")

print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ:")
if final_val_acc < 80:
    print(f"   - ë°ì´í„° ìˆ˜ì§‘ í™•ëŒ€")
    print(f"   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    print(f"   - ë” ê°•ë ¥í•œ ë°ì´í„° ì¦ê°•")
    print(f"   - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¡°ì •")
elif final_val_acc < 85:
    print(f"   - ì•™ìƒë¸” ëª¨ë¸ ì‹œë„")
    print(f"   - ì¶”ê°€ ë¯¸ì„¸ì¡°ì •")
    print(f"   - í…ŒìŠ¤íŠ¸ íƒ€ì„ ì¦ê°•(TTA)")
elif final_val_acc < 90:
    print(f"   - ëª¨ë¸ ì•™ìƒë¸”")
    print(f"   - ì‹¤ì œ ì„œë¹„ìŠ¤ ì ìš© í…ŒìŠ¤íŠ¸")
    print(f"   - ì¶”ë¡  ìµœì í™”")
else:
    print(f"   - ì‹¤ì œ ì„œë¹„ìŠ¤ ë°°í¬ ì¤€ë¹„")
    print(f"   - ëª¨ë¸ ê²½ëŸ‰í™” ë° ìµœì í™”")
    print(f"   - ì‹¤ì‹œê°„ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ êµ¬ì¶•")
    print(f"   - A/B í…ŒìŠ¤íŠ¸ ì¤€ë¹„")

print(f"\nğŸ‰ AffectNet ê¸°ë°˜ ì–¼êµ´ í‘œì • ë¶„ë¥˜ í•™ìŠµ ì™„ë£Œ!")
print(f"ğŸ“ ëª¨ë“  ê²°ê³¼ê°€ {results_base}/ ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ğŸš€ ë…¼ë¬¸ì˜ í•µì‹¬ ê¸°ë²•ì„ ì„±ê³µì ìœ¼ë¡œ ì ìš©í–ˆìŠµë‹ˆë‹¤!")
print(f"   - âœ… AffectNet ì‚¬ì „í•™ìŠµ ëª¨ë¸ í™œìš©")
print(f"   - âœ… penultimate layer íŠ¹ì§• ì¶”ì¶œ (1408ì°¨ì›)")
print(f"   - âœ… 2ë‹¨ê³„ ì „ì´í•™ìŠµ (ë™ê²° â†’ í•´ì œ)")
print(f"   - âœ… ì „ì²˜ë¦¬ëœ ì •ë ¬ ì–¼êµ´ ì´ë¯¸ì§€ ì‚¬ìš©")
print(f"   - âœ… Mixed Precision Training")
print(f"   - âœ… í¬ê´„ì ì¸ ì„±ëŠ¥ ë¶„ì„ ë° ì‹œê°í™”")

# =================================
# ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ ìƒì„±
# =================================
inference_code = f'''
# í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ
import torch
from PIL import Image
from torchvision import transforms

# ëª¨ë¸ ë¡œë”©
model = AffectNetBinaryClassifier(affectnet_model, feature_dim={feature_dim})
model.load_state_dict(torch.load('{final_model_path}'))
model.eval()
model.to(device)

# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
preprocess = transforms.Compose([
    transforms.Resize((260, 260)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# ì¶”ë¡  í•¨ìˆ˜
def predict_emotion(image_path):
    image = Image.open(image_path).convert('RGB')
    input_tensor = preprocess(image).unsqueeze(0).to(device)
    
    with torch.no_grad():
        outputs = model(input_tensor)
        probabilities = torch.softmax(outputs, dim=1)
        predicted_class = torch.argmax(probabilities, dim=1).item()
        confidence = probabilities[0][predicted_class].item()
    
    class_names = {classes}
    return class_names[predicted_class], confidence

# ì‚¬ìš© ì˜ˆì‹œ
# result, confidence = predict_emotion('path/to/face_image.jpg')
# print(f"ì˜ˆì¸¡: {{result}}, ì‹ ë¢°ë„: {{confidence:.3f}}")
'''

inference_code_path = f"{results_base}/inference_example_{timestamp}.py"
with open(inference_code_path, 'w', encoding='utf-8') as f:
    f.write(inference_code)
print(f"ğŸ“ ì¶”ë¡  ì˜ˆì‹œ ì½”ë“œ ì €ì¥: {inference_code_path}")