#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import cv2
import yaml
import time
import json
import threading
import queue
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Optional

import numpy as np
from PIL import Image
import tensorflow as tf
from tensorflow.keras.applications.efficientnet import preprocess_input

from mtcnn_wrapper import FaceDetector


class CalmmanVideoProcessor:
    """
    ì¹¨ì°©ë§¨ í‚¹ë°›ëŠ” ìˆœê°„ íƒì§€ë¥¼ ìœ„í•œ ë¹„ë””ì˜¤ í”„ë¡œì„¸ì„œ
    """
    
    def __init__(self, config_path: str = "config/config.yaml"):
        """
        ë¹„ë””ì˜¤ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        
        Args:
            config_path (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        # ì„¤ì • ë¡œë“œ
        self.config = self._load_config(config_path)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë³€ìˆ˜
        self.stats = {
            'frames_processed': 0,
            'faces_detected': 0,
            'angry_moments': 0,
            'processing_start_time': None,
            'last_stats_time': time.time()
        }
        
        # ì¢…ë£Œ í”Œë˜ê·¸ ì¶”ê°€
        self.stop_flag = False
        self.face_detection_done = False
        self.classification_done = False
        
        # í ìƒì„±
        self.frame_queue = queue.Queue(maxsize=self.config['performance']['max_queue_size'])
        self.face_queue = queue.Queue(maxsize=self.config['performance']['max_queue_size'])
        self.result_queue = queue.Queue()
        
        # ì–¼êµ´ íƒì§€ê¸° ì´ˆê¸°í™”
        self._init_face_detector()
        
        # ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ
        self._load_classifier()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_output_dirs()
        
        print("âœ… CalmmanVideoProcessor ì´ˆê¸°í™” ì™„ë£Œ")
        self._print_config_summary()
    
    def _load_config(self, config_path: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ: {config_path}")
            return config
        except Exception as e:
            print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _init_face_detector(self):
        """MTCNN ì–¼êµ´ íƒì§€ê¸° ì´ˆê¸°í™”"""
        mtcnn_config = self.config['mtcnn']
        
        self.face_detector = FaceDetector(
            image_size=mtcnn_config['image_size'],
            margin=mtcnn_config['margin'],
            prob_threshold=mtcnn_config['prob_threshold'],
            align_faces=mtcnn_config['align_faces']
        )
        
        print(f"âœ… MTCNN ì´ˆê¸°í™” ì™„ë£Œ (ë°°ì¹˜ í¬ê¸°: {mtcnn_config['batch_size']})")
    
    def _load_classifier(self):
        """ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ"""
        try:
            model_path = self.config['classifier']['model_path']
            self.classifier = tf.keras.models.load_model(model_path)
            print(f"âœ… ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ: {model_path}")
        except Exception as e:
            print(f"âŒ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _create_output_dirs(self):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        if self.config['output']['save_highlights']:
            os.makedirs(self.config['output']['highlights_dir'], exist_ok=True)
        
        if self.config['output']['save_timestamps']:
            os.makedirs(self.config['output']['timestamps_dir'], exist_ok=True)
    
    def _print_config_summary(self):
        """ì„¤ì • ìš”ì•½ ì¶œë ¥"""
        print("\nğŸ“‹ ì„¤ì • ìš”ì•½:")
        print(f"   í”„ë ˆì„ ìŠ¤í‚µ: {self.config['video']['frame_skip']}í”„ë ˆì„ë§ˆë‹¤")
        print(f"   MTCNN ë°°ì¹˜: {self.config['mtcnn']['batch_size']}")
        print(f"   ë¶„ë¥˜ ì›Œì»¤: {self.config['classifier']['workers']}ê°œ")
        print(f"   í í¬ê¸°: {self.config['performance']['max_queue_size']}")
        print(f"   RAM ì œí•œ: {self.config['performance']['max_ram_gb']}GB")
        print()
    
    def process_video(self, video_path: str) -> Dict:
        """
        ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
        
        Args:
            video_path (str): ì²˜ë¦¬í•  ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
        """
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        print(f"\nğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘: {os.path.basename(video_path)}")
        
        # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        video_info = self._get_video_info(video_path)
        print(f"   ê¸¸ì´: {video_info['duration']:.1f}ì´ˆ, FPS: {video_info['fps']:.1f}")
        
        # ê²°ê³¼ ì €ì¥ìš© ë° í”Œë˜ê·¸ ì´ˆê¸°í™”
        self.angry_moments = []
        self.stats['processing_start_time'] = time.time()
        self.face_detection_done = False
        self.classification_done = False
        self.stop_flag = False
        
        # ìŠ¤ë ˆë“œ ì‹œì‘
        threads = []
        
        # 1. í”„ë ˆì„ ì½ê¸° ìŠ¤ë ˆë“œ
        frame_thread = threading.Thread(
            target=self._frame_reader_worker, 
            args=(video_path,)
        )
        threads.append(frame_thread)
        
        # 2. ì–¼êµ´ íƒì§€ ìŠ¤ë ˆë“œ
        face_thread = threading.Thread(target=self._face_detection_worker)
        threads.append(face_thread)
        
        # 3. ë¶„ë¥˜ ìŠ¤ë ˆë“œ
        classify_thread = threading.Thread(target=self._classification_worker)
        threads.append(classify_thread)
        
        # 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ (ë°ëª¬ìœ¼ë¡œ ì„¤ì •)
        monitor_thread = threading.Thread(target=self._performance_monitor)
        monitor_thread.daemon = True  # ë°ëª¬ ìŠ¤ë ˆë“œë¡œ ì„¤ì •
        threads.append(monitor_thread)
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì‹œì‘
        for thread in threads:
            thread.start()
        
        # í”„ë ˆì„ ì½ê¸° ì™„ë£Œ ëŒ€ê¸°
        frame_thread.join()
        print("âœ… í”„ë ˆì„ ì½ê¸° ì™„ë£Œ")
        
        # ë‘ ì‘ì—… ëª¨ë‘ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        print("â³ ì–¼êµ´ íƒì§€ ë° ë¶„ë¥˜ ì™„ë£Œ ëŒ€ê¸° ì¤‘...")
        while not (self.face_detection_done and self.classification_done):
            time.sleep(0.1)
        
        print("âœ… ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")
        
        # ì¢…ë£Œ í”Œë˜ê·¸ ì„¤ì •
        self.stop_flag = True
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œë“¤ ì •ë¦¬
        face_thread.join(timeout=2)
        classify_thread.join(timeout=2)
        
        # ê²°ê³¼ ì €ì¥
        results = self._save_results(video_path, video_info)
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        self._print_final_stats()
        
        return results
    
    def _get_video_info(self, video_path: str) -> Dict:
        """ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ"""
        cap = cv2.VideoCapture(video_path)
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = frame_count / fps if fps > 0 else 0
        
        cap.release()
        
        return {
            'fps': fps,
            'frame_count': frame_count,
            'duration': duration
        }
    
    def _frame_reader_worker(self, video_path: str):
        """í”„ë ˆì„ ì½ê¸° ì›Œì»¤"""
        cap = cv2.VideoCapture(video_path)
        frame_skip = self.config['video']['frame_skip']
        frame_count = 0
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # í”„ë ˆì„ ìŠ¤í‚µ
                if frame_count % frame_skip != 0:
                    frame_count += 1
                    continue
                
                # BGR to RGB ë³€í™˜
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°
                timestamp = frame_count / cap.get(cv2.CAP_PROP_FPS)
                
                # íì— ì¶”ê°€
                frame_data = {
                    'frame': frame_rgb,
                    'frame_number': frame_count,
                    'timestamp': timestamp
                }
                
                self.frame_queue.put(frame_data)
                frame_count += 1
                
        except Exception as e:
            print(f"âŒ í”„ë ˆì„ ì½ê¸° ì˜¤ë¥˜: {e}")
        finally:
            cap.release()
            # ì¢…ë£Œ ì‹ í˜¸
            self.frame_queue.put(None)
            print("ğŸ“¹ í”„ë ˆì„ ì½ê¸° ì›Œì»¤ ì¢…ë£Œ")
    
    def _face_detection_worker(self):
        """ì–¼êµ´ íƒì§€ ì›Œì»¤"""
        batch_size = self.config['mtcnn']['batch_size']
        frame_batch = []
        
        try:
            while True:
                frame_data = self.frame_queue.get()
                
                if frame_data is None:  # ì¢…ë£Œ ì‹ í˜¸
                    # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
                    if frame_batch:
                        self._process_face_batch(frame_batch)
                    self.face_queue.put(None)  # ë‹¤ìŒ ì›Œì»¤ì— ì¢…ë£Œ ì‹ í˜¸
                    break
                
                frame_batch.append(frame_data)
                
                # ë°°ì¹˜ê°€ ì°¼ìœ¼ë©´ ì²˜ë¦¬
                if len(frame_batch) >= batch_size:
                    self._process_face_batch(frame_batch)
                    frame_batch = []
                
                self.frame_queue.task_done()
                
        except Exception as e:
            print(f"âŒ ì–¼êµ´ íƒì§€ ì˜¤ë¥˜: {e}")
        finally:
            self.face_detection_done = True  # ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •
            print("âœ… ì–¼êµ´ íƒì§€ ì™„ë£Œ")
    
    def _process_face_batch(self, frame_batch: List[Dict]):
        """í”„ë ˆì„ ë°°ì¹˜ì—ì„œ ì–¼êµ´ íƒì§€"""
        for frame_data in frame_batch:
            try:
                # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
                pil_image = Image.fromarray(frame_data['frame'])
                
                # ì–¼êµ´ íƒì§€
                face_images = self.face_detector.process_image(pil_image)
                
                if face_images:
                    for face_img in face_images:
                        face_data = {
                            'face_image': face_img,
                            'frame_number': frame_data['frame_number'],
                            'timestamp': frame_data['timestamp']
                        }
                        self.face_queue.put(face_data)
                        self.stats['faces_detected'] += 1
                
                self.stats['frames_processed'] += 1
                
            except Exception as e:
                print(f"âš ï¸ í”„ë ˆì„ {frame_data['frame_number']} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _classification_worker(self):
        """ë¶„ë¥˜ ì›Œì»¤"""
        try:
            while True:
                face_data = self.face_queue.get()
                
                if face_data is None:  # ì¢…ë£Œ ì‹ í˜¸
                    break
                
                try:
                    # ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
                    img_array = np.array(face_data['face_image'])
                    img_array = img_array.astype('float32')  # [0,255] ìœ ì§€
                    img_array = np.expand_dims(img_array, axis=0)

                    # EfficientNet ì „ìš© ì •ê·œí™”
                    img_array = preprocess_input(img_array)
                    
                    # ë¶„ë¥˜ ì˜ˆì¸¡
                    prediction = self.classifier.predict(img_array, verbose=0)
                    confidence = float(prediction[0][0])  # í‚¹ë°›ëŠ” í™•ë¥ 
                    
                    # ì„ê³„ê°’ í™•ì¸
                    threshold = self.config['classifier']['confidence_threshold']
                    is_angry = confidence > threshold
                    
                    if is_angry:
                        angry_moment = {
                            'timestamp': face_data['timestamp'],
                            'frame_number': face_data['frame_number'],
                            'confidence': confidence
                        }
                        
                        self.angry_moments.append(angry_moment)
                        self.stats['angry_moments'] += 1
                        
                        # í‚¹ë°›ëŠ” í”„ë ˆì„ ì €ì¥ (ì˜µì…˜)
                        if self.config['output']['save_highlights']:
                            self._save_highlight_image(face_data, confidence)
                        
                        if self.config['debug']['verbose']:
                            timestamp_str = str(timedelta(seconds=int(face_data['timestamp'])))
                            print(f"ğŸ˜¡ í‚¹ë°›ëŠ” ìˆœê°„ ë°œê²¬! {timestamp_str} (ì‹ ë¢°ë„: {confidence:.3f})")
                
                except Exception as e:
                    print(f"âš ï¸ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
                
                self.face_queue.task_done()
                
        except Exception as e:
            print(f"âŒ ë¶„ë¥˜ ì›Œì»¤ ì˜¤ë¥˜: {e}")
        finally:
            self.classification_done = True  # ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •
            print("âœ… ë¶„ë¥˜ ì²˜ë¦¬ ì™„ë£Œ")
    
    def _save_highlight_image(self, face_data: Dict, confidence: float):
        """í‚¹ë°›ëŠ” ìˆœê°„ ì´ë¯¸ì§€ ì €ì¥"""
        try:
            timestamp_str = f"{int(face_data['timestamp']):05d}"
            filename = f"angry_{timestamp_str}_{confidence:.3f}.jpg"
            
            save_path = os.path.join(
                self.config['output']['highlights_dir'],
                filename
            )
            
            face_data['face_image'].save(save_path)
            
        except Exception as e:
            print(f"âš ï¸ í•˜ì´ë¼ì´íŠ¸ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _performance_monitor(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        interval = self.config['performance']['monitoring_interval']
        
        while not self.stop_flag:  # ì¢…ë£Œ í”Œë˜ê·¸ í™•ì¸
            time.sleep(interval)
            
            if self.stats['processing_start_time'] is None:
                continue
            
            # í˜„ì¬ í†µê³„
            elapsed = time.time() - self.stats['processing_start_time']
            fps = self.stats['frames_processed'] / elapsed if elapsed > 0 else 0
            
            # í ìƒíƒœ
            frame_queue_size = self.frame_queue.qsize()
            face_queue_size = self.face_queue.qsize()
            
            if self.config['debug']['performance_log']:
                print(f"ğŸ“Š [{elapsed:.1f}s] "
                      f"í”„ë ˆì„: {self.stats['frames_processed']} "
                      f"({fps:.1f} FPS), "
                      f"ì–¼êµ´: {self.stats['faces_detected']}, "
                      f"í‚¹ë°›ìŒ: {self.stats['angry_moments']}, "
                      f"í: {frame_queue_size}/{face_queue_size}")
        
        print("ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")
    
    def _save_results(self, video_path: str, video_info: Dict) -> Dict:
        """ê²°ê³¼ ì €ì¥"""
        results = {
            'video_path': video_path,
            'video_info': video_info,
            'processing_stats': self.stats.copy(),
            'angry_moments': self.angry_moments,
            'total_angry_moments': len(self.angry_moments)
        }
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ JSON ì €ì¥
        if self.config['output']['save_timestamps']:
            video_name = Path(video_path).stem
            timestamp_file = os.path.join(
                self.config['output']['timestamps_dir'],
                f"{video_name}_angry_moments.json"
            )
            
            with open(timestamp_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {timestamp_file}")
        
        return results
    
    def _print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        elapsed = time.time() - self.stats['processing_start_time']
        fps = self.stats['frames_processed'] / elapsed if elapsed > 0 else 0
        
        print(f"\nğŸ¯ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
        print(f"   ì²˜ë¦¬ëœ í”„ë ˆì„: {self.stats['frames_processed']}ê°œ ({fps:.1f} FPS)")
        print(f"   íƒì§€ëœ ì–¼êµ´: {self.stats['faces_detected']}ê°œ")
        print(f"   í‚¹ë°›ëŠ” ìˆœê°„: {self.stats['angry_moments']}ê°œ")
        print()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    # ê¸°ë³¸ ì„¤ì •
    video_path = "yt_download/downloads/ì›ƒìŒì´ í•„ìš”í•  ë•Œ ë³´ëŠ” í‚¹ë°›ëŠ” ì¹¨ì°©ë§¨.mp4"
    
    try:
        # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        processor = CalmmanVideoProcessor()
        
        # ë¹„ë””ì˜¤ ì²˜ë¦¬
        results = processor.process_video(video_path)
        
        print("âœ… ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ê°•ì œ ì¢…ë£Œ
        print("ğŸ”š í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(0)


if __name__ == "__main__":
    main()